# -*- coding: utf-8 -*-
"""Human Post-Estimation for Sports Action.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10yjMlz9J4yEOwFkqIsPotUDCDLde-3qj

# Human Post-Estimation for Sports Action

##**Domain**: Computer Vision
"""

!pip install opencv-python mediapipe numpy matplotlib scikit-learn

!pip install opencv-python mediapipe numpy matplotlib scipy scikit-image scikit-learn pycocotools

"""Human Pose Estimation for Sports Action - COCO Dataset Integration
Production-Level Computer Vision Project with Advanced Techniques
Direct COCO Dataset Integration with Image and Annotation Loading
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Circle, Rectangle, Polygon
from scipy import signal, ndimage
from collections import deque
import urllib.request
import os
import json
import warnings
from pathlib import Path
import zipfile
warnings.filterwarnings('ignore')

class COCODatasetLoader:
    """Load and manage COCO dataset for pose estimation"""

    def __init__(self, coco_path='coco_dataset', auto_download=True):
        """
        Initialize COCO dataset loader
        Args:
            coco_path: Directory to store COCO dataset
            auto_download: Automatically download if missing
        """
        self.coco_path = coco_path
        self.annotations_path = os.path.join(coco_path, 'annotations')
        self.images_path = os.path.join(coco_path, 'images')

        # Create directories
        os.makedirs(self.annotations_path, exist_ok=True)
        os.makedirs(self.images_path, exist_ok=True)

        self.coco_api = None
        self.image_ids = []
        self.annotations_data = None

        if auto_download:
            self._setup_coco_dataset()

    def _setup_coco_dataset(self):
        """Setup and download COCO dataset if needed"""
        print("üîç Checking COCO dataset files...")

        try:
            from pycocotools.coco import COCO

            # Check multiple possible locations
            possible_paths = [
                os.path.join(self.coco_path, 'annotations', 'person_keypoints_val2017.json'),
                os.path.join(self.annotations_path, 'person_keypoints_val2017.json'),
            ]

            annotations_file = None
            for path in possible_paths:
                if os.path.exists(path):
                    annotations_file = path
                    print(f"‚úì Found annotations: {annotations_file}")
                    break

            # Download if not found
            if annotations_file is None:
                print("üì• Downloading COCO annotations...")
                self._download_coco_annotations()

                # Check again after download
                for path in possible_paths:
                    if os.path.exists(path):
                        annotations_file = path
                        break

            if annotations_file is None:
                print("‚ùå Could not find or download annotations")
                print("üìù Manual setup instructions:")
                print(f"1. Download: http://images.cocodataset.org/annotations/annotations_trainval2017.zip")
                print(f"2. Extract to: {self.coco_path}")
                print(f"3. Ensure file exists: {possible_paths[0]}")
                return

            # Load COCO API
            print(f"üìÇ Loading COCO from: {annotations_file}")
            self.coco_api = COCO(annotations_file)

            # Get image IDs with person keypoints
            cat_ids = self.coco_api.getCatIds(catNms=['person'])
            self.image_ids = self.coco_api.getImgIds(catIds=cat_ids)

            print(f"‚úì COCO loaded: {len(self.image_ids)} person images found")

        except ImportError:
            print("‚ùå pycocotools not found. Installing...")
            os.system('pip install pycocotools')
            import time
            time.sleep(2)
            self._setup_coco_dataset()
        except Exception as e:
            print(f"‚ùå Error loading COCO: {e}")
            print("Continuing without COCO dataset...")

    def _download_coco_annotations(self):
        """Download COCO annotations (lightweight)"""
        print("Downloading COCO 2017 Val annotations (~50MB)...")
        url = "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"

        zip_path = os.path.join(self.coco_path, 'coco_annotations.zip')

        try:
            urllib.request.urlretrieve(url, zip_path)
            print("‚úì Download complete, extracting...")

            # Extract to coco_path (it will create 'annotations' folder)
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(self.coco_path)

            # Check extracted structure
            extracted_ann_path = os.path.join(self.coco_path, 'annotations')
            if os.path.exists(extracted_ann_path):
                print(f"‚úì Annotations extracted to: {extracted_ann_path}")

            os.remove(zip_path)
            print("‚úì Annotations setup complete")

        except Exception as e:
            print(f"‚ö† Error downloading: {e}")
            print(f"Manual download: {url}")
            print(f"Extract to: {self.coco_path}")

    def get_sample_images(self, num_samples=10, person_only=True):
        """
        Get sample images from COCO dataset
        Returns: List of image paths and their annotations
        """
        if not self.image_ids or self.coco_api is None:
            print("‚ö† COCO not properly loaded")
            return []

        # Get images with keypoints annotations
        ann_ids = self.coco_api.getAnnIds(catIds=[1])  # Category 1 = person
        all_anns = self.coco_api.loadAnns(ann_ids)

        # Filter annotations that have keypoints
        anns_with_keypoints = [ann for ann in all_anns
                               if ann.get('num_keypoints', 0) > 0
                               and len(ann.get('keypoints', [])) > 0]

        if not anns_with_keypoints:
            print("‚ö† No keypoint annotations found")
            return []

        # Sample random annotations
        num_to_sample = min(num_samples, len(anns_with_keypoints))
        sampled_anns = np.random.choice(anns_with_keypoints, num_to_sample, replace=False)

        samples = []
        seen_images = set()

        for ann in sampled_anns:
            try:
                img_id = ann['image_id']

                # Skip if we already processed this image
                if img_id in seen_images:
                    continue
                seen_images.add(img_id)

                img_data = self.coco_api.loadImgs(img_id)[0]
                img_path = os.path.join(self.images_path, img_data['file_name'])

                # Get all annotations for this image
                img_ann_ids = self.coco_api.getAnnIds(imgIds=img_id, catIds=[1])
                img_annotations = self.coco_api.loadAnns(img_ann_ids)

                # Filter for annotations with keypoints
                person_anns = [a for a in img_annotations
                              if a.get('num_keypoints', 0) > 0]

                if not person_anns:
                    continue

                samples.append({
                    'image_id': img_id,
                    'image_path': img_path,
                    'image_data': img_data,
                    'annotations': person_anns,
                    'num_persons': len(person_anns),
                    'num_keypoints': sum(a.get('num_keypoints', 0) for a in person_anns)
                })

                if len(samples) >= num_samples:
                    break

            except Exception as e:
                print(f"‚ö† Error loading image: {e}")
                continue

        print(f"‚úì Found {len(samples)} images with keypoint annotations")
        return samples

    def download_image(self, img_data, save_path):
        """Download image from COCO URL"""
        try:
            url = img_data.get('coco_url') or img_data.get('flickr_url')
            if not url:
                # Construct URL from file_name
                file_name = img_data['file_name']
                # COCO 2017 val images URL
                url = f"http://images.cocodataset.org/val2017/{file_name}"

            print(f"    Downloading from COCO: {os.path.basename(save_path)}...", end=' ')

            # Add headers to avoid 403 errors
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})

            with urllib.request.urlopen(req, timeout=10) as response:
                with open(save_path, 'wb') as out_file:
                    out_file.write(response.read())

            print("‚úì")
            return True

        except Exception as e:
            print(f"‚úó ({e})")
            return False

    def get_keypoint_info(self):
        """Get COCO keypoint information"""
        return {
            'names': ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
                     'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
                     'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
                     'left_knee', 'right_knee', 'left_ankle', 'right_ankle'],
            'skeleton': [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13],
                        [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],
                        [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],
                        [2, 4], [3, 5], [4, 6], [5, 7]]
        }

    def extract_keypoints_from_coco(self, annotation):
        """Extract keypoint data from COCO annotation"""
        keypoints_array = annotation.get('keypoints', [])

        if not keypoints_array:
            return []

        keypoints = []
        for i in range(0, len(keypoints_array), 3):
            x, y, v = keypoints_array[i], keypoints_array[i+1], keypoints_array[i+2]

            if v > 0:  # v=0 means keypoint not visible
                keypoints.append({
                    'idx': i // 3,
                    'x': x,
                    'y': y,
                    'visibility': v
                })

        return keypoints

class AdvancedImageProcessing:
    """Advanced image enhancement and feature extraction techniques"""

    @staticmethod
    def histogram_equalization_adaptive(frame, clip_limit=2.0):
        """Adaptive Histogram Equalization (CLAHE)"""
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)

        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))
        l = clahe.apply(l)

        enhanced = cv2.merge([l, a, b])
        return cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)

    @staticmethod
    def morphological_processing(frame):
        """Morphological operations: Opening and Closing"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))

        opened = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)
        processed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)

        return processed

    @staticmethod
    def edge_detection_canny(frame, sigma=0.33):
        """Canny Edge Detection with automatic threshold"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        v = np.median(gray)

        lower = int(max(0, (1.0 - sigma) * v))
        upper = int(min(255, (1.0 + sigma) * v))

        edges = cv2.Canny(gray, lower, upper)
        return edges

    @staticmethod
    def corner_detection_harris(frame, block_size=2, ksize=3, k=0.04):
        """Harris Corner Detection"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = np.float32(gray)

        corners = cv2.cornerHarris(gray, block_size, ksize, k)
        corners = cv2.dilate(corners, None)

        return corners

class RANSACLandmarkFilter:
    """RANSAC-based filtering for robust landmark detection"""

    @staticmethod
    def filter_landmarks_ransac(landmarks, n_iterations=100, threshold=10):
        """RANSAC for outlier rejection"""
        if len(landmarks) < 4:
            return landmarks

        points = np.array([[lm['x'], lm['y']] for lm in landmarks])

        best_inliers = []
        best_model = None

        for _ in range(n_iterations):
            idx = np.random.choice(len(points), 2, replace=False)
            p1, p2 = points[idx]

            if np.allclose(p1[0], p2[0]):
                continue

            slope = (p2[1] - p1[1]) / (p2[0] - p1[0] + 1e-6)
            intercept = p1[1] - slope * p1[0]

            distances = np.abs(points[:, 1] - (slope * points[:, 0] + intercept)) / np.sqrt(1 + slope**2)
            inliers = np.sum(distances < threshold)

            if inliers > len(best_inliers):
                best_inliers = np.where(distances < threshold)[0]
                best_model = (slope, intercept)

        if len(best_inliers) == 0:
            return landmarks

        filtered = [landmarks[i] for i in best_inliers]
        return filtered

class AdvancedPoseEstimationWithCOCO:
    """Advanced pose estimation with COCO dataset integration"""

    def __init__(self, model_complexity=1, min_detection_confidence=0.7):
        """Initialize with COCO dataset support"""
        try:
            import mediapipe as mp
        except ImportError:
            os.system('pip install mediapipe')
            import mediapipe as mp

        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        self.pose = self.mp_pose.Pose(
            static_image_mode=True,
            model_complexity=model_complexity,
            smooth_landmarks=False,
            min_detection_confidence=min_detection_confidence
        )

        self.keypoint_names = self._get_keypoint_names()
        self.coco_loader = COCODatasetLoader()
        self.detection_stats = {
            'total_processed': 0,
            'successful_detections': 0,
            'action_distribution': {}
        }

    def _get_keypoint_names(self):
        """MediaPipe keypoint names"""
        return {
            0: 'nose', 11: 'left_shoulder', 12: 'right_shoulder',
            13: 'left_elbow', 14: 'right_elbow', 15: 'left_wrist', 16: 'right_wrist',
            23: 'left_hip', 24: 'right_hip', 25: 'left_knee', 26: 'right_knee',
            27: 'left_ankle', 28: 'right_ankle'
        }

    def preprocess_frame(self, frame):
        """Apply advanced image preprocessing"""
        enhanced = AdvancedImageProcessing.histogram_equalization_adaptive(frame)
        return enhanced

    def detect_pose_from_image(self, frame):
        """Detect pose in single image"""
        preprocessed = self.preprocess_frame(frame)

        frame_rgb = cv2.cvtColor(preprocessed, cv2.COLOR_BGR2RGB)
        results = self.pose.process(frame_rgb)

        annotated_frame = frame.copy()
        landmarks = []

        if results.pose_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.pose_landmarks,
                self.mp_pose.POSE_CONNECTIONS,
                self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),
                self.mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)
            )

            h, w, _ = frame.shape
            for idx, lm in enumerate(results.pose_landmarks.landmark):
                landmarks.append({
                    'idx': idx,
                    'name': self.keypoint_names.get(idx, f'point_{idx}'),
                    'x': lm.x * w,
                    'y': lm.y * h,
                    'z': lm.z,
                    'confidence': lm.visibility
                })

        # Apply RANSAC filtering
        filtered_landmarks = RANSACLandmarkFilter.filter_landmarks_ransac(landmarks)

        return filtered_landmarks, annotated_frame

    def calculate_angle(self, point_a, point_b, point_c):
        """Calculate angle between three points"""
        a = np.array([point_a['x'], point_a['y']])
        b = np.array([point_b['x'], point_b['y']])
        c = np.array([point_c['x'], point_c['y']])

        ba = a - b
        bc = c - b

        cos_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)
        angle = np.arccos(np.clip(cos_angle, -1, 1))
        return np.degrees(angle)

    def detect_action(self, landmarks):
        """Classify sports action"""
        if len(landmarks) < 10:
            return "No person detected", 0.0

        lm_dict = {lm['name']: lm for lm in landmarks}

        required_points = ['left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
                          'left_wrist', 'right_wrist', 'left_knee', 'right_knee',
                          'left_ankle', 'right_ankle']

        if not all(p in lm_dict for p in required_points):
            return "Incomplete pose", 0.0

        left_elbow_angle = self.calculate_angle(
            lm_dict['left_shoulder'], lm_dict['left_elbow'], lm_dict['left_wrist']
        )
        right_elbow_angle = self.calculate_angle(
            lm_dict['right_shoulder'], lm_dict['right_elbow'], lm_dict['right_wrist']
        )
        left_knee_angle = self.calculate_angle(
            lm_dict['left_hip'], lm_dict['left_knee'], lm_dict['left_ankle']
        )
        right_knee_angle = self.calculate_angle(
            lm_dict['right_hip'], lm_dict['right_knee'], lm_dict['right_ankle']
        )

        shoulder_width = abs(lm_dict['left_shoulder']['x'] - lm_dict['right_shoulder']['x'])
        shoulder_height_diff = abs(lm_dict['left_shoulder']['y'] - lm_dict['right_shoulder']['y'])

        if (left_knee_angle < 90 or right_knee_angle < 90) and shoulder_height_diff < 30:
            return "Squatting", 0.85
        elif (left_elbow_angle < 100 or right_elbow_angle < 100) and shoulder_height_diff > 50:
            return "Throwing", 0.80
        elif shoulder_height_diff > 80 and (left_elbow_angle > 150 or right_elbow_angle > 150):
            return "Defending", 0.82
        elif (left_knee_angle > 150 and right_knee_angle > 150) and shoulder_width > 100:
            return "Standing", 0.90
        else:
            return "Running/Moving", 0.75

    def process_coco_dataset(self, num_samples=10, visualize=True):
        """Process images from COCO dataset"""
        print(f"\nüìä Processing {num_samples} COCO dataset samples...")

        # Check if COCO is loaded
        if self.coco_loader.coco_api is None or len(self.coco_loader.image_ids) == 0:
            print("‚ö† COCO dataset not available. Using demo mode with sample images.")
            return self._demo_mode_processing(num_samples)

        samples = self.coco_loader.get_sample_images(num_samples=num_samples)

        if not samples:
            print("‚ö† No COCO samples found. Using demo mode.")
            return self._demo_mode_processing(num_samples)

        results = []

        for i, sample in enumerate(samples):
            try:
                # Download image if needed
                img_path = sample['image_path']

                if not os.path.exists(img_path):
                    print(f"  Downloading image {i+1}/{len(samples)}...")
                    self.coco_loader.download_image(sample['image_data'], img_path)

                if not os.path.exists(img_path):
                    print(f"‚ö† Skipping sample {i+1}: Image not available")
                    continue

                # Read and process image
                frame = cv2.imread(img_path)
                if frame is None:
                    continue

                # Detect pose
                landmarks, annotated_frame = self.detect_pose_from_image(frame)
                action, confidence = self.detect_action(landmarks)

                # Update statistics
                self.detection_stats['total_processed'] += 1
                if landmarks:
                    self.detection_stats['successful_detections'] += 1

                action_key = action.split('/')[0].strip()
                self.detection_stats['action_distribution'][action_key] = \
                    self.detection_stats['action_distribution'].get(action_key, 0) + 1

                # Extract COCO keypoints for comparison
                coco_keypoints = []
                for ann in sample['annotations']:
                    coco_kps = self.coco_loader.extract_keypoints_from_coco(ann)
                    coco_keypoints.extend(coco_kps)

                result = {
                    'image_id': sample['image_id'],
                    'image_path': img_path,
                    'detected_landmarks': landmarks,
                    'coco_keypoints': coco_keypoints,
                    'detected_action': action,
                    'confidence': confidence,
                    'annotated_frame': annotated_frame
                }

                results.append(result)

                print(f"‚úì Sample {i+1}/{len(samples)}: {action} (confidence: {confidence:.2f}, "
                      f"landmarks: {len(landmarks)})")

            except Exception as e:
                print(f"‚ö† Error processing sample {i+1}: {e}")
                continue

        # Print statistics
        print(f"\nüìà Processing Statistics:")
        print(f"  Total processed: {self.detection_stats['total_processed']}")
        print(f"  Successful detections: {self.detection_stats['successful_detections']}")
        print(f"  Success rate: {self.detection_stats['successful_detections']/max(self.detection_stats['total_processed'],1)*100:.1f}%")
        print(f"  Actions detected: {self.detection_stats['action_distribution']}")

        return results

    def _demo_mode_processing(self, num_samples=5):
        """Demo mode with sample images when COCO is unavailable"""
        print("\nüé® Demo Mode: Processing sample images...")
        print("Note: Creating synthetic test images for demonstration...")

        results = []
        demo_path = os.path.join(self.coco_loader.coco_path, 'demo_images')
        os.makedirs(demo_path, exist_ok=True)


        for idx in range(min(num_samples, 3)):
            try:
                img_path = os.path.join(demo_path, f'demo_{idx}.jpg')


                height, width = 480, 640
                frame = np.zeros((height, width, 3), dtype=np.uint8)
                frame[:] = (240, 240, 240)


                center_x, center_y = width // 2, height // 2


                cv2.circle(frame, (center_x, center_y - 100), 30, (255, 200, 150), -1)


                cv2.line(frame, (center_x, center_y - 70), (center_x, center_y + 80), (100, 100, 200), 8)


                cv2.line(frame, (center_x, center_y - 40), (center_x - 60, center_y + 20), (100, 100, 200), 6)
                cv2.line(frame, (center_x, center_y - 40), (center_x + 60, center_y + 20), (100, 100, 200), 6)


                cv2.line(frame, (center_x, center_y + 80), (center_x - 40, center_y + 160), (100, 100, 200), 6)
                cv2.line(frame, (center_x, center_y + 80), (center_x + 40, center_y + 160), (100, 100, 200), 6)


                cv2.putText(frame, f'Demo Person {idx+1}', (20, 40),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)

                cv2.imwrite(img_path, frame)


                landmarks, annotated_frame = self.detect_pose_from_image(frame)
                action, confidence = self.detect_action(landmarks)

                result = {
                    'image_id': f'demo_{idx}',
                    'image_path': img_path,
                    'detected_landmarks': landmarks,
                    'coco_keypoints': [],
                    'detected_action': action,
                    'confidence': confidence,
                    'annotated_frame': annotated_frame
                }

                results.append(result)
                print(f"‚úì Demo {idx+1}: {action} (confidence: {confidence:.2f}, landmarks: {len(landmarks)})")

            except Exception as e:
                print(f"‚ö† Demo image {idx+1} failed: {e}")
                continue

        if not results:
            print("‚ö† Demo mode failed. Please provide your own images.")

        return results

    def visualize_coco_comparison(self, result, figsize=(16, 6)):
        """Visualize COCO keypoints vs detected keypoints"""
        fig, axes = plt.subplots(1, 3, figsize=figsize)

        frame = cv2.imread(result['image_path'])
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)


        axes[0].imshow(frame_rgb)
        axes[0].set_title('Original Image')
        axes[0].axis('off')


        axes[1].imshow(frame_rgb)
        for lm in result['detected_landmarks']:
            if lm['confidence'] > 0.5:
                axes[1].add_patch(Circle((lm['x'], lm['y']), 5, color='red', fill=False))
        axes[1].set_title(f"Detected: {result['detected_action']} ({result['confidence']:.2f})")
        axes[1].axis('off')


        axes[2].imshow(frame_rgb)
        for kp in result['coco_keypoints']:
            axes[2].add_patch(Circle((kp['x'], kp['y']), 5, color='blue', fill=False))
        axes[2].set_title(f"COCO Keypoints: {len(result['coco_keypoints'])} detected")
        axes[2].axis('off')

        plt.tight_layout()
        plt.show()

    def visualize_batch_results(self, results, num_show=6):
        """Visualize batch of results"""
        num_show = min(num_show, len(results))
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()

        for idx in range(num_show):
            result = results[idx]
            annotated_frame = result['annotated_frame']
            annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)

            axes[idx].imshow(annotated_frame_rgb)
            axes[idx].set_title(f"{result['detected_action']}\nConf: {result['confidence']:.2f}")
            axes[idx].axis('off')

        plt.tight_layout()
        plt.show()

# ===================== MAIN EXECUTION =====================

if __name__ == "__main__":
    print("üéØ Advanced Pose Estimation with COCO Dataset Integration")
    print("=" * 70)
    print("Features:")
    print("  ‚úì COCO Dataset Loading and Management")
    print("  ‚úì Adaptive Histogram Equalization (CLAHE)")
    print("  ‚úì Morphological Processing")
    print("  ‚úì Canny Edge Detection")
    print("  ‚úì Harris Corner Detection")
    print("  ‚úì RANSAC Landmark Filtering")
    print("  ‚úì Sports Action Classification")
    print("=" * 70)


    pose_estimator = AdvancedPoseEstimationWithCOCO(model_complexity=1)

    print("\nSetting up COCO dataset...")


    if pose_estimator.coco_loader.coco_api and pose_estimator.coco_loader.image_ids:
        print(f"‚úì COCO dataset ready with {len(pose_estimator.coco_loader.image_ids)} person images")


        print("\nGetting sample images info...")
        samples = pose_estimator.coco_loader.get_sample_images(num_samples=5)

        if samples:
            print(f"\n‚úì Found {len(samples)} samples ready for processing")
            print("\nSample details:")
            for i, sample in enumerate(samples[:3]):
                print(f"  {i+1}. Image ID: {sample['image_id']}, "
                      f"Persons: {sample['num_persons']}, "
                      f"Keypoints: {sample['num_keypoints']}")


            print(f"\nüîÑ Downloading and processing {min(3, len(samples))} COCO images...")
            print("   (This will download real person images from COCO dataset)")

            results = pose_estimator.process_coco_dataset(num_samples=3, visualize=False)

        else:
            print("‚ö† Could not find samples with keypoints")
            results = []
    else:
        print("‚ö† COCO dataset not loaded")
        results = []

    if results and len(results) > 0:
        print(f"\n‚úÖ Successfully processed {len(results)} COCO images!")
        print("\nüìä Results Summary:")
        for i, result in enumerate(results):
            print(f"  {i+1}. Action: {result['detected_action']}, "
                  f"Confidence: {result['confidence']:.2f}, "
                  f"Landmarks: {len(result['detected_landmarks'])}, "
                  f"COCO KPs: {len(result['coco_keypoints'])}")


        print("\nVisualizing first result with COCO comparison...")
        try:
            pose_estimator.visualize_coco_comparison(results[0])
        except Exception as e:
            print(f"‚ö† Visualization skipped: {e}")

        print("\nVisualizing all results...")
        try:
            pose_estimator.visualize_batch_results(results, num_show=min(3, len(results)))
        except Exception as e:
            print(f"‚ö† Batch visualization skipped: {e}")
    else:
        print("\n‚ö† No results obtained. Note: MediaPipe requires real human images.")
        print("   Stick figures won't work - need actual photos of people.")

    print("\n" + "=" * 70)
    print("‚úÖ COCO Dataset Integration Complete!")
    print("\nüìù Quick Usage:")
    print("\n1. Process more COCO images (downloads automatically):")
    print("   results = pose_estimator.process_coco_dataset(num_samples=10)")
    print("\n2. Process your own image file:")
    print("   img = cv2.imread('your_image.jpg')")
    print("   landmarks, annotated = pose_estimator.detect_pose_from_image(img)")
    print("   action, conf = pose_estimator.detect_action(landmarks)")
    print("\n3. Visualize comparison:")
    print("   pose_estimator.visualize_coco_comparison(results[0])")
    print("\n4. Get COCO samples info (no download):")
    print("   samples = pose_estimator.coco_loader.get_sample_images(10)")
    print("\nüí° Note: MediaPipe requires real human photos (not drawings/sketches)")
    print("   COCO images will be downloaded automatically when processing.")

results = pose_estimator.process_coco_dataset(num_samples=10)

img = cv2.imread('Human interaction.png')
landmarks, annotated = pose_estimator.detect_pose_from_image(img)
action, conf = pose_estimator.detect_action(landmarks)


pose_estimator.visualize_coco_comparison(results[0])


samples = pose_estimator.coco_loader.get_sample_images(10)

